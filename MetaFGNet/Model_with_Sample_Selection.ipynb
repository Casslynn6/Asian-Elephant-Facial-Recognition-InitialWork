{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from models.resnet import resnet  # The model construction\n",
    "import torchvision\n",
    "from trainer import train\n",
    "# Configuration Options\n",
    "from utils import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_resnet34_/share/jproject/fg508/mchivuku/datasets/asian_elephant/models/metafgnet/target_128Timg_/share/jproject/fg508/mchivuku/datasets/asian_elephant/models/metafgnet/source_128Simg_Meta_train_Lr0.001_1'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "d = get_config(config=\"configs/elephant.yaml\")\n",
    "d[\"log\"] =\"\"\n",
    "args = namedtuple(\"args\", d.keys())(*d.values())\n",
    "log = args.log + '_' + args.arch + '_' + args.dataset + '_' + str(args.batch_size) + 'Timg_' + args.auxiliary_dataset \\\n",
    "               + '_' + str(args.batch_size_source) + 'Simg_Meta_train_Lr' + str(args.meta_train_lr) + '_' +\\\n",
    "               str(args.num_updates_for_gradient)\n",
    "log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from data.folder_new import ImageFolder_new\n",
    "\n",
    "def generate_dataloader(target_train_path, target_val_path, aux_train_path, aux_val_path):\n",
    "    \n",
    "    # Data loading code\n",
    "    # the dataloader for the target dataset.\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_dataset = ImageFolder_new(\n",
    "        target_train_path,\n",
    "        transforms.Compose([\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.RandomCrop(224),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers = args.workers, pin_memory=True, sampler=None\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        ImageFolder_new(target_val_path, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    # the dataloader for the source dataset.\n",
    "    traindir_source = aux_train_path\n",
    "    valdir_source = aux_val_path\n",
    "    \n",
    "    if len(os.listdir(traindir_source)) != 0:\n",
    "        normalize_source = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                std=[0.229, 0.224, 0.225])\n",
    "        train_dataset_source = ImageFolder_new(\n",
    "            traindir_source,\n",
    "            transforms.Compose([\n",
    "                #transforms.Resize(256),\n",
    "                #transforms.RandomCrop(224),\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize_source,\n",
    "            ])\n",
    "        )\n",
    "        train_loader_source = torch.utils.data.DataLoader(\n",
    "            train_dataset_source, batch_size=args.batch_size_source, shuffle=True,\n",
    "            num_workers=args.workers, pin_memory=True, sampler=None\n",
    "        )\n",
    "\n",
    "        val_loader_source = torch.utils.data.DataLoader(\n",
    "            ImageFolder_new(valdir_source, transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize_source,\n",
    "            ])),\n",
    "            batch_size=args.batch_size_source, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True\n",
    "        )\n",
    "        print(f\"No of train source:{len(train_loader_source)}, No of source val: {len(val_loader_source)},No. of target train :{len(train_loader)}, No of target val: {len(val_loader)}\")\n",
    "        return train_loader_source, val_loader_source, train_loader, val_loader\n",
    "    else:\n",
    "        return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of train source:107, No of source val: 46,No. of target train :1, No of target val: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f77760a9a90>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f7775fba6d8>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f77761cea58>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f77760a9a20>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_dataloader(os.path.join(args.data_path,\"train\"), os.path.join(args.data_path,\"val\"), os.path.join(args.auxiliary_dataset,\"train\"),os.path.join(args.auxiliary_dataset,\"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> creating model 'resnet34' \n",
      "load the imagenet pretrained model resnet34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /N/u/mchivuku/.torch/models/resnet34-333f7ec4.pth\n",
      "100%|██████████| 87306240/87306240 [00:01<00:00, 67211628.08it/s]\n"
     ]
    }
   ],
   "source": [
    "model_source, model_target = resnet(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define-multi GPU\n",
    "model_source = torch.nn.DataParallel(model_source).cuda()\n",
    "model_target = torch.nn.DataParallel(model_target).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the memory id should be same for the shared feature extractor:\n",
      "140151057157088\n",
      "140151057157088\n"
     ]
    }
   ],
   "source": [
    "print('the memory id should be same for the shared feature extractor:')\n",
    "print(id(model_source.module.resnet_conv))   # the memory is shared here\n",
    "print(id(model_target.module.resnet_conv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the memory id should be different for the different classifiers:\n",
      "140151058174416\n",
      "140151058175984\n"
     ]
    }
   ],
   "source": [
    "print('the memory id should be different for the different classifiers:')\n",
    "print(id(model_source.module.fc))  # the memory id shared here.\n",
    "print(id(model_target.module.fc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function(criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "np.random.seed(1)  ### fix the random data.\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Meta-SGD to update parameteres\n",
    "if args.meta_sgd:\n",
    "    meta_train_lr = []\n",
    "    for param in model_target.parameters():\n",
    "        meta_train_lr.append(torch.FloatTensor(param.data.size()).fill_(args.meta_train_lr).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pretrained ## Pretrained settings for Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([\n",
    "            {'params': model_source.module.resnet_conv.parameters(), 'name': 'new-added'},\n",
    "            {'params': model_source.module.fc.parameters(), 'name': 'new-added'},\n",
    "            {'params': model_target.module.fc.parameters(), 'name': 'new-added'},\n",
    "        ],\n",
    "                                    lr=args.learning_rate,\n",
    "                                    momentum=args.momentum,\n",
    "                                    weight_decay=args.weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If Resume is true\n",
    "global best_prec1\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"==> loading checkpoints '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        if args.meta_sgd:\n",
    "            meta_train_lr = checkpoint['meta_train_lr']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model_source.load_state_dict(checkpoint['source_state_dict'])\n",
    "        model_target.load_state_dict(checkpoint['target_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"==> loaded checkpoint '{}'(epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        raise ValueError('The file to be resumed from is not exited', args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(args.log_folder):\n",
    "    os.makedirs(args.log_folder)\n",
    "log = open(os.path.join(args.log_folder, 'log.txt'), 'w')\n",
    "state = {k: v for k, v in d.items()}\n",
    "log.write(json.dumps(state) + '\\n')\n",
    "log.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of train source:107, No of source val: 46,No. of target train :1, No of target val: 1\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "dataloader_returned = generate_dataloader(os.path.join(args.data_path,\"train\"), os.path.join(args.data_path,\"val\"), os.path.join(args.auxiliary_dataset,\"train\"),os.path.join(args.auxiliary_dataset,\"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_number_returned = len(dataloader_returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of dataloader 4 returned   \n"
     ]
    }
   ],
   "source": [
    "print(f'the number of dataloader {dataloader_number_returned} returned   ' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataloader_number_returned != 2:\n",
    "    train_loader_source, val_loader_source, train_loader_target, val_loader_target = dataloader_returned\n",
    "else:\n",
    "    train_loader_target, val_loader_target = dataloader_returned\n",
    "    train_loader_source = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n"
     ]
    }
   ],
   "source": [
    "print('begin training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_loader_source:\n",
    "    train_loader_source_batch = enumerate(train_loader_source)\n",
    "else:\n",
    "    train_loader_source_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_target_batch = enumerate(train_loader_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/mchivuku/asian_elephant_facial_recognition/MetaFGNet/trainer.py:159: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  losses_target.update(loss_target.data[0], input_target_mtrain.size(0))\n",
      "/N/u/mchivuku/asian_elephant_facial_recognition/MetaFGNet/trainer.py:176: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  losses_source.update(loss_source.data[0], input_source.size(0))\n",
      "/N/u/mchivuku/asian_elephant_facial_recognition/MetaFGNet/trainer.py:181: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  losses_real.update(real_loss.data[0], input_source.size(0) + input_target_mtrain.size(0))   # here the index for the loss is  input_source.size(0), may be not properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr epoch [0/10000]\tBT 132.167 (132.167)\tDT 100.885 (100.885)\tLoss 5.7286 (5.7286)\tS@1 7.031 (7.031)\tS@5 32.812 (32.812)\tLS 2.8835 (2.8835)\tT@1 6.400 (6.400)\tT@5 34.400 (34.400)\tLT 2.8451 (2.8451)\n",
      "Tr epoch [1/10000]\tBT 87.855 (87.855)\tDT 86.755 (86.755)\tLoss 7.2402 (7.2402)\tS@1 12.500 (12.500)\tS@5 50.000 (50.000)\tLS 3.7062 (3.7062)\tT@1 36.000 (36.000)\tT@5 51.200 (51.200)\tLT 3.5340 (3.5340)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (63) : OS call failed or operation not supported on this OS at /pytorch/aten/src/THC/THCTensorCopy.cu:102",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-e7b98c77f2ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_loader_source_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_target_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_train_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_source_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader_target_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_train_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_loader_source_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_target_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_source_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader_target_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# train(train_loader, model, criterion, optimizer, epoch, args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# evaluate on the val data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/asian_elephant_facial_recognition/MetaFGNet/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader_source, train_loader_source_batch, train_loader_target, train_loader_target_batch, model_source, model_target, criterion, optimizer, epoch, args, meta_train_lr)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mmeta_train_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_target_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_train_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0moutput_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_target_temp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_target_mtrain_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mloss_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_target_mtrain_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mmodel_target_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Setting the function to None clears the refcycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgather_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mgather_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mgather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mGather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueezed_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/selfensembling/lib/python3.6/site-packages/torch/cuda/comm.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mconcatenating\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0malong\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (63) : OS call failed or operation not supported on this OS at /pytorch/aten/src/THC/THCTensorCopy.cu:102"
     ]
    }
   ],
   "source": [
    "print('begin training')\n",
    "if train_loader_source:\n",
    "    train_loader_source_batch = enumerate(train_loader_source)\n",
    "else:\n",
    "    train_loader_source_batch = None\n",
    "train_loader_target_batch = enumerate(train_loader_target)\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if args.meta_sgd:\n",
    "        train_loader_source_batch, train_loader_target_batch, meta_train_lr = train(train_loader_source, train_loader_source_batch, train_loader_target,train_loader_target_batch, model_source, model_target, criterion, optimizer, epoch, args, meta_train_lr)\n",
    "    else:\n",
    "        train_loader_source_batch, train_loader_target_batch = train(train_loader_source, train_loader_source_batch, train_loader_target,train_loader_target_batch, model_source, model_target, criterion, optimizer, epoch, args, None)\n",
    "    # train(train_loader, model, criterion, optimizer, epoch, args)\n",
    "    # evaluate on the val data\n",
    "    if (epoch + 1) % args.test_freq == 0 or (epoch + 1) % args.epochs == 0:\n",
    "        if dataloader_number_returned == 2:\n",
    "            prec1 = validate(None, val_loader_target, model_source, model_target, criterion, epoch, args)\n",
    "        else:\n",
    "            prec1 = validate(val_loader_source, val_loader_target, model_source, model_target, criterion, epoch, args)\n",
    "        # prec1 = 1\n",
    "        # record the best prec1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        if is_best:\n",
    "            log = open(os.path.join(args.log, 'log.txt'), 'a')\n",
    "            log.write('     \\nTarget_T1 acc: %3f' % (best_prec1))\n",
    "            log.close()\n",
    "        if args.meta_sgd:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'meta_train_lr': meta_train_lr,\n",
    "                'arch': args.arch,\n",
    "                'source_state_dict': model_source.state_dict(),\n",
    "                'target_state_dict': model_target.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, is_best, args, epoch)\n",
    "        else:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': args.arch,\n",
    "                'source_state_dict': model_source.state_dict(),\n",
    "                'target_state_dict': model_target.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, is_best, args, epoch + 1)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, args, epoch):\n",
    "    filename = str(epoch) + 'checkpoint.pth.tar'\n",
    "    dir_save_file = os.path.join(args.log, filename)\n",
    "    torch.save(state, dir_save_file)\n",
    "    if is_best:\n",
    "        shutil.copyfile(dir_save_file, os.path.join(args.log, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2500, 5000, 7500, 9000]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
